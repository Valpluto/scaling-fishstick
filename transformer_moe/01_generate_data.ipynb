{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start using the `tensor2tensor` models, we first have to get our data into a format that `tensor2tensor` can digest. This means defining a custom `Problem` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this only once - Sets up TF Eager execution.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Enable Eager execution - useful for seeing the generated data.\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0802 18:59:01.531362 139639908783936 deprecation_wrapper.py:119] From /home/aa5118/anaconda3/envs/tf/lib/python3.7/site-packages/tensor2tensor/utils/expert_utils.py:68: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0802 18:59:02.064932 139639908783936 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0802 18:59:02.987431 139639908783936 deprecation_wrapper.py:119] From /home/aa5118/anaconda3/envs/tf/lib/python3.7/site-packages/tensor2tensor/utils/metrics_hook.py:28: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "W0802 18:59:02.991913 139639908783936 deprecation_wrapper.py:119] From /home/aa5118/anaconda3/envs/tf/lib/python3.7/site-packages/tensor2tensor/utils/adafactor.py:27: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0802 18:59:02.992884 139639908783936 deprecation_wrapper.py:119] From /home/aa5118/anaconda3/envs/tf/lib/python3.7/site-packages/tensor2tensor/utils/multistep_optimizer.py:32: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0802 18:59:03.003361 139639908783936 deprecation_wrapper.py:119] From /home/aa5118/anaconda3/envs/tf/lib/python3.7/site-packages/tensor2tensor/utils/trainer_lib.py:109: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.\n",
      "\n",
      "W0802 18:59:03.006350 139639908783936 deprecation_wrapper.py:119] From /home/aa5118/anaconda3/envs/tf/lib/python3.7/site-packages/tensor2tensor/utils/trainer_lib.py:780: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Setting a random seed.\n",
    "\n",
    "from tensor2tensor.utils import trainer_lib\n",
    "\n",
    "# Set a seed so that we have deterministic outputs.\n",
    "RANDOM_SEED = 301\n",
    "trainer_lib.set_random_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run for setting up directories.\n",
    "\n",
    "import os\n",
    "\n",
    "# Setup and create directories.\n",
    "DATA_DIR = os.path.expanduser(\"../data/t2t_experiments/transformer_moe/full_context/data\")\n",
    "OUTPUT_DIR = os.path.expanduser(\"../data/t2t_experiments/transformer_moe/full_context/output\")\n",
    "TMP_DIR = os.path.expanduser(\"/mnt/\")\n",
    "\n",
    "# Create them.\n",
    "tf.gfile.MakeDirs(DATA_DIR)\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "tf.gfile.MakeDirs(TMP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "from tensor2tensor.utils import registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@registry.register_problem\n",
    "\n",
    "class MimicDischargeSummaries(text_problems.Text2TextProblem):\n",
    "    \n",
    "    @property\n",
    "    def is_generate_per_split(self):\n",
    "        # our data already has pre-existing splits so we return true\n",
    "        return True\n",
    "\n",
    "    def generate_samples(self, data_dir, tmp_dir, dataset_split):\n",
    "        \n",
    "        del tmp_dir\n",
    "        \n",
    "        _train = (dataset_split == problem.DatasetSplit.TRAIN)\n",
    "        _eval = (dataset_split == problem.DatasetSplit.EVAL)\n",
    "        \n",
    "        dataset = \"train\" if _train else \"val\" if _eval else \"test\"\n",
    "        \n",
    "        full_context = \"full_context\" in str(data_dir) # returns a boolean\n",
    "        directory = \"../data/preprocessed/\"\n",
    "        tgt = directory + \"tgt-\" + dataset + \".txt\"\n",
    "\n",
    "        if full_context == True:\n",
    "            src = directory + \"src-\" + dataset + \".txt\"\n",
    "        else:\n",
    "            directory += \"other_contexts/\" \n",
    "            context = str(data_dir)[39:-5] # this index needs to be changed if file paths are changed\n",
    "            src = directory + \"src-\" + dataset + \"-\" + context + \".txt\"\n",
    "        \n",
    "        f_src = open(src,'r')\n",
    "        f_tgt = open(tgt,'r')\n",
    "        \n",
    "        context_data = f_src.readline()\n",
    "        discharge_summary = f_tgt.readline()\n",
    "\n",
    "        while context_data:\n",
    "            yield {\n",
    "              \"inputs\"  : context_data,\n",
    "              \"targets\" : discharge_summary,\n",
    "            }\n",
    "            \n",
    "            context_data = f_src.readline()\n",
    "            discharge_summary = f_tgt.readline()\n",
    "            \n",
    "        f_src.close()\n",
    "        f_tgt.close()\n",
    "\n",
    "    @property\n",
    "    def vocab_type(self):\n",
    "        # SUBWORD and CHARACTER are fully invertible -- but SUBWORD provides a good\n",
    "        # tradeoff between CHARACTER and TOKEN.\n",
    "        return text_problems.VocabType.SUBWORD\n",
    "\n",
    "    @property\n",
    "    def approx_vocab_size(self):\n",
    "        # Approximate vocab size to generate. Only for VocabType.SUBWORD.\n",
    "        return 2**15  # ~32k - this is the default setting\n",
    "\n",
    "    @property\n",
    "    def dataset_splits(self):\n",
    "        return [{\n",
    "            \"split\": problem.DatasetSplit.TRAIN,\n",
    "            \"shards\": 80\n",
    "        }, {\n",
    "            \"split\": problem.DatasetSplit.EVAL,\n",
    "            \"shards\": 10\n",
    "        }, {\n",
    "            \"split\": problem.DatasetSplit.TEST,\n",
    "            \"shards\": 10\n",
    "        }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we instantiate the problem and run it for the full context data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_problem = MimicDischargeSummaries()\n",
    "#mimic_problem.generate_data(DATA_DIR, TMP_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we run it in a loop instead for each individual context type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 19:05:31.212889 139639908783936 deprecation_wrapper.py:119] From /home/aa5118/anaconda3/envs/tf/lib/python3.7/site-packages/tensor2tensor/data_generators/generator_utils.py:343: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n",
      "W0802 19:05:31.213634 139639908783936 deprecation_wrapper.py:119] From /home/aa5118/anaconda3/envs/tf/lib/python3.7/site-packages/tensor2tensor/data_generators/generator_utils.py:349: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "W0802 19:08:39.032947 139639908783936 deprecation_wrapper.py:119] From /home/aa5118/anaconda3/envs/tf/lib/python3.7/site-packages/tensor2tensor/data_generators/generator_utils.py:355: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n",
      "W0802 19:08:39.034780 139639908783936 deprecation_wrapper.py:119] From /home/aa5118/anaconda3/envs/tf/lib/python3.7/site-packages/tensor2tensor/data_generators/text_encoder.py:944: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W0802 19:08:39.085365 139639908783936 deprecation_wrapper.py:119] From /home/aa5118/anaconda3/envs/tf/lib/python3.7/site-packages/tensor2tensor/data_generators/generator_utils.py:164: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "W0802 19:11:46.960594 139639908783936 deprecation_wrapper.py:119] From /home/aa5118/anaconda3/envs/tf/lib/python3.7/site-packages/tensor2tensor/data_generators/generator_utils.py:183: The name tf.gfile.Rename is deprecated. Please use tf.io.gfile.rename instead.\n",
      "\n",
      "W0802 19:12:35.876786 139639908783936 deprecation.py:323] From /home/aa5118/anaconda3/envs/tf/lib/python3.7/site-packages/tensor2tensor/data_generators/generator_utils.py:469: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "W0802 19:12:35.887185 139639908783936 deprecation_wrapper.py:119] From /home/aa5118/anaconda3/envs/tf/lib/python3.7/site-packages/tensor2tensor/data_generators/generator_utils.py:513: The name tf.gfile.Remove is deprecated. Please use tf.io.gfile.remove instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_list = ['h','h-gae','h-gae-d','h-gae-p','h-gae-d-p','h-gae-d-p-m','h-gae-d-p-m-t','h-gae-d-p-m-l']\n",
    "\n",
    "for context in context_list:\n",
    "    # Setup and create directories.\n",
    "    DATA_DIR = os.path.expanduser(\"../data/t2t_experiments/other_contexts/\"+context+\"/data\")\n",
    "    OUTPUT_DIR = os.path.expanduser(\"../data/t2t_experiments/other_contexts/\"+context+\"/output\")\n",
    "    TMP_DIR = os.path.expanduser(\"/mnt/\")\n",
    "\n",
    "    # Create them.\n",
    "    tf.gfile.MakeDirs(DATA_DIR)\n",
    "    tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "    tf.gfile.MakeDirs(TMP_DIR)\n",
    "    \n",
    "    mimic_problem.generate_data(DATA_DIR, TMP_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0729 17:59:46.342158 140696631490368 deprecation_wrapper.py:119] From /home/aa5118/anaconda3/envs/tf/lib/python3.7/site-packages/tensor2tensor/data_generators/text_problems.py:394: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
      "\n",
      "W0729 17:59:46.343035 140696631490368 deprecation_wrapper.py:119] From /home/aa5118/anaconda3/envs/tf/lib/python3.7/site-packages/tensor2tensor/data_generators/problem.py:705: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Input: tf.Tensor(\n",
      "[   80    72    54  1451     6    42     6   121    18    55    72   398\n",
      "   368     3   228     4   365     3   269     4   364     3  4213     4\n",
      "   366     3  2468  3414  1737  1534  2069     7  9304    60    95     7\n",
      "   659   825   329    60   518    77   661     7  6604    14  5907    15\n",
      "   101  2002     7  4855   832  4317     7  3060    60    95    60   132\n",
      "   438    15  3903  3060     7    95   732   234     7   101   661   179\n",
      "  3527     7   469    15   101   180   131     4   358   308   367     3\n",
      "    36     9    45   846   208   291     5   124   275   289     7   349\n",
      "   291     5    97    48  2648   281     7   369  2174     5    21   136\n",
      "    30   124   275   889   289     4   228     3   720    11    85     4\n",
      "   337     3   237   243     5 10137     5   116     6    83     5    29\n",
      "     7   214     5  1005     5    48     6    38     7   252   106   112\n",
      "     5    27     9  1342     5   114     6    83     5    29     7   266\n",
      "     5   134     9    41   123    29     7   486     5   306     9    41\n",
      "     5   223     7   221     5   107     9    42     5   223     5    29\n",
      "     7   263     5   887     5   264     7   260     5   174     9    45\n",
      "   109   261     5   181     9    39     5   255     5    29     7   459\n",
      "   404   221   156    16     9    21     5    85     5    29     7   238\n",
      "     5    45     9    41     5   136     6    40     5    29     7   202\n",
      "     5    89     9    36   123    29     7   226   227     5    41     5\n",
      "    20     6    40     7   258   106   112     5    39     9    32     5\n",
      "   116     6    83     7   195     5    33     9    36     5    48     6\n",
      "    38     7   280     5    21     9    16     5    20     6    40     7\n",
      "   197     5   800     5    20     6    40     7   220     5    16     9\n",
      "    36     5    20     6    40     7   225     5  1184     5    48     6\n",
      "    38     7   283    60   178     5    39     9    32     5    20     6\n",
      "    40     7   324     5    27     9    32     5    20     6    40     7\n",
      "   250   249     5    64     5    48     6    38     7   241     5   115\n",
      "     5    48     6    38     7   241     5   172     5    48     6    38\n",
      "     7   225     5  1039     5    48     6    38     7   283    60   178\n",
      "     5    39     9    27     5    20     6    40     5    29     7   258\n",
      "   106   112     5    45     9    32     5   116     6    83     7   250\n",
      "   249     5    52     5    48     6    38     7   197     5  2597     5\n",
      "    20     6    40     5    29     7   252   106   112     5    27     9\n",
      "  1299     5   114     6    83     5    29     7   220     5    16     9\n",
      "    36     5    20     6    40     7   214     5   867     5    48     6\n",
      "    38     7   280     5    21     9    36     5    20     6    40     7\n",
      "   324     5    27     9    32     5    20     6    40     7   195     5\n",
      "    27     9    42     5    48     6    38     7   226   227     5    41\n",
      "     5    20     6    40     7   202     5   174     9    16   123    29\n",
      "     7   238     5    52     9    36     5   136     6    40     5    29\n",
      "     7   261     5   181     9    45     5   255     5    29     7   260\n",
      "     5   253     9    16   109   263     5   887     5   264     7   266\n",
      "     5   134     9    21   123    29     7   237   243     5  9035     5\n",
      "   116     6    83     5    29     4    38   370     1], shape=(537,), dtype=int64)\n",
      "Tensor Target: tf.Tensor([ 80  72  54 ...   2 606   1], shape=(3837,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "tfe = tf.contrib.eager\n",
    "\n",
    "Modes = tf.estimator.ModeKeys\n",
    "\n",
    "# We can iterate over our examples by making an iterator and calling next on it.\n",
    "eager_iterator = tfe.Iterator(mimic_problem.dataset(Modes.EVAL, DATA_DIR))\n",
    "example = eager_iterator.next()\n",
    "\n",
    "input_tensor = example[\"inputs\"]\n",
    "target_tensor = example[\"targets\"]\n",
    "\n",
    "# The tensors are actually encoded using the generated vocabulary file -- you\n",
    "# can inspect the actual vocab file in DATA_DIR.\n",
    "print(\"Tensor Input: \" + str(input_tensor))\n",
    "print(\"Tensor Target: \" + str(target_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell is not executed in order to protect patient privacy. Executing it will show the decoded context data and discharge summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the encoders to decode the tensors to the actual input text.\n",
    "input_encoder = mimic_problem.get_feature_encoders(\n",
    "    data_dir=DATA_DIR)[\"inputs\"]\n",
    "target_encoder = mimic_problem.get_feature_encoders(\n",
    "    data_dir=DATA_DIR)[\"targets\"]\n",
    "\n",
    "input_decoded = input_encoder.decode(input_tensor.numpy())\n",
    "target_decoded = target_encoder.decode(target_tensor.numpy())\n",
    "\n",
    "print(\"Decoded Input: \" + input_decoded)\n",
    "print(\"Decoded Target: \" + target_decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
