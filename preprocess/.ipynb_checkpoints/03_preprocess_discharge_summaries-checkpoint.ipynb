{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Discharge Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T13:30:09.285361894Z",
     "start_time": "2024-02-15T13:30:09.243401934Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "import string\n",
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "import scispacy\n",
    "from collections import Counter\n",
    "import re\n",
    "from datetime import date, datetime, timedelta\n",
    "import random\n",
    "from sklearn.model_selection import GroupShuffleSplit, StratifiedShuffleSplit\n",
    "#from spellchecker import SpellChecker\n",
    "#from flashtext import KeywordProcessor\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the mimic database and set the search path to the 'mimiciii' schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T13:30:10.314755680Z",
     "start_time": "2024-02-15T13:30:10.310113937Z"
    }
   },
   "outputs": [],
   "source": [
    "dbschema='mimiciii'\n",
    "cnx = sqlalchemy.create_engine('postgresql+psycopg2://aa5118:mimic@10.45.20.20:5432/mimic',\n",
    "                    connect_args={'options': '-csearch_path={}'.format(dbschema)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the discharge summary notes joined on to patient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T13:30:14.860012452Z",
     "start_time": "2024-02-15T13:30:11.255275927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   subject_id        dob gender  hadm_id           category  chartdate  \\\n0           3 2025-04-11      M   145834  Discharge summary 2101-10-31   \n1           4 2143-05-12      F   185777  Discharge summary 2191-03-23   \n2           6 2109-06-21      F   107064  Discharge summary 2175-06-15   \n3           9 2108-01-26      M   150750  Discharge summary 2149-11-14   \n4           9 2108-01-26      M   150750  Discharge summary 2149-11-13   \n\n   row_id  age_at_noteevent                                               text  \n0   44005              77.0  Admission Date:  [**2101-10-20**]     Discharg...  \n1    4788              48.0  Admission Date:  [**2191-3-16**]     Discharge...  \n2   20825              66.0  Admission Date: [**2175-5-30**]        Dischar...  \n3   57115              42.0  Name:  [**Known lastname 10050**], [**Known fi...  \n4   20070              42.0  Admission Date:  [**2149-11-9**]       Dischar...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject_id</th>\n      <th>dob</th>\n      <th>gender</th>\n      <th>hadm_id</th>\n      <th>category</th>\n      <th>chartdate</th>\n      <th>row_id</th>\n      <th>age_at_noteevent</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>2025-04-11</td>\n      <td>M</td>\n      <td>145834</td>\n      <td>Discharge summary</td>\n      <td>2101-10-31</td>\n      <td>44005</td>\n      <td>77.0</td>\n      <td>Admission Date:  [**2101-10-20**]     Discharg...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>2143-05-12</td>\n      <td>F</td>\n      <td>185777</td>\n      <td>Discharge summary</td>\n      <td>2191-03-23</td>\n      <td>4788</td>\n      <td>48.0</td>\n      <td>Admission Date:  [**2191-3-16**]     Discharge...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6</td>\n      <td>2109-06-21</td>\n      <td>F</td>\n      <td>107064</td>\n      <td>Discharge summary</td>\n      <td>2175-06-15</td>\n      <td>20825</td>\n      <td>66.0</td>\n      <td>Admission Date: [**2175-5-30**]        Dischar...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>2108-01-26</td>\n      <td>M</td>\n      <td>150750</td>\n      <td>Discharge summary</td>\n      <td>2149-11-14</td>\n      <td>57115</td>\n      <td>42.0</td>\n      <td>Name:  [**Known lastname 10050**], [**Known fi...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9</td>\n      <td>2108-01-26</td>\n      <td>M</td>\n      <td>150750</td>\n      <td>Discharge summary</td>\n      <td>2149-11-13</td>\n      <td>20070</td>\n      <td>42.0</td>\n      <td>Admission Date:  [**2149-11-9**]       Dischar...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "  SELECT\n",
    "      p.subject_id, p.dob, p.gender,\n",
    "      n.hadm_id, n.category, n.chartdate, n.row_id,\n",
    "      ROUND((cast(chartdate as date) - cast(dob as date)) / 365.242,0)\n",
    "          AS age_at_noteevent,\n",
    "      n.text\n",
    "  FROM patients p \n",
    "  INNER JOIN noteevents n \n",
    "  ON p.subject_id = n.subject_id\n",
    "  WHERE ROUND((cast(chartdate as date) - cast(dob as date)) / 365.242,0) > 14\n",
    "  AND n.category = 'Discharge summary'\n",
    "  ORDER BY subject_id, chartdate DESC\n",
    "  --LIMIT 25000\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(sqlalchemy.text(sql), cnx)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change data type of age to the smallest possible type of integer to save memory and get rid of decimal point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T13:30:14.878188415Z",
     "start_time": "2024-02-15T13:30:14.858646760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    subject_id        dob gender  hadm_id           category  chartdate  \\\n0            3 2025-04-11      M   145834  Discharge summary 2101-10-31   \n1            4 2143-05-12      F   185777  Discharge summary 2191-03-23   \n2            6 2109-06-21      F   107064  Discharge summary 2175-06-15   \n3            9 2108-01-26      M   150750  Discharge summary 2149-11-14   \n4            9 2108-01-26      M   150750  Discharge summary 2149-11-13   \n5           11 2128-02-22      F   194540  Discharge summary 2178-05-11   \n6           12 2032-03-24      M   112213  Discharge summary 2104-08-20   \n7           13 2127-02-27      F   143045  Discharge summary 2167-01-15   \n8           13 2127-02-27      F   143045  Discharge summary 2167-01-15   \n9           17 2087-07-14      F   161087  Discharge summary 2135-05-13   \n10          17 2087-07-14      F   194023  Discharge summary 2134-12-31   \n11          18 2116-11-29      M   188822  Discharge summary 2167-10-04   \n12          18 2116-11-29      M   188822  Discharge summary 2167-10-02   \n13          18 2116-11-29      M   188822  Discharge summary 2167-10-02   \n14          19 1808-08-05      M   109235  Discharge summary 2108-08-11   \n15          20 2107-06-13      F   157681  Discharge summary 2183-05-03   \n16          20 2107-06-13      F   157681  Discharge summary 2183-04-28   \n17          21 2047-04-04      M   111970  Discharge summary 2135-02-08   \n18          21 2047-04-04      M   109451  Discharge summary 2134-09-24   \n19          22 2131-05-07      F   165315  Discharge summary 2196-04-10   \n\n    row_id  age_at_noteevent  \\\n0    44005                77   \n1     4788                48   \n2    20825                66   \n3    57115                42   \n4    20070                42   \n5    30120                50   \n6    50972                72   \n7    20168                40   \n8    57099                40   \n9    51783                48   \n10   51782                47   \n11    7824                51   \n12   55875                51   \n13    7823                51   \n14   25231               300   \n15   17861                76   \n16   17860                76   \n17    7238                88   \n18    7237                87   \n19   12144                65   \n\n                                                 text  \n0   Admission Date:  [**2101-10-20**]     Discharg...  \n1   Admission Date:  [**2191-3-16**]     Discharge...  \n2   Admission Date: [**2175-5-30**]        Dischar...  \n3   Name:  [**Known lastname 10050**], [**Known fi...  \n4   Admission Date:  [**2149-11-9**]       Dischar...  \n5   Admission Date:  [**2178-4-16**]              ...  \n6   Admission Date:  [**2104-8-7**]     Discharge ...  \n7   Admission Date:  [**2167-1-8**]       Discharg...  \n8   Name:  [**Known lastname 9900**], [**Known fir...  \n9   Admission Date:  [**2135-5-9**]              D...  \n10  Admission Date:  [**2134-12-27**]             ...  \n11  Admission Date:  [**2167-10-2**]              ...  \n12  Name:  [**Known lastname 4919**] JR,[**Known f...  \n13  Admission Date:  [**2167-9-30**]              ...  \n14  Admission Date:  [**2108-8-5**]              D...  \n15  Admission Date:  [**2183-4-28**]       Dischar...  \n16  Admission Date:  [**2183-5-10**]       Dischar...  \n17  Admission Date:  [**2135-1-30**]              ...  \n18  Admission Date:  [**2134-9-11**]              ...  \n19  Admission Date:  [**2196-4-9**]       Discharg...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject_id</th>\n      <th>dob</th>\n      <th>gender</th>\n      <th>hadm_id</th>\n      <th>category</th>\n      <th>chartdate</th>\n      <th>row_id</th>\n      <th>age_at_noteevent</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>2025-04-11</td>\n      <td>M</td>\n      <td>145834</td>\n      <td>Discharge summary</td>\n      <td>2101-10-31</td>\n      <td>44005</td>\n      <td>77</td>\n      <td>Admission Date:  [**2101-10-20**]     Discharg...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>2143-05-12</td>\n      <td>F</td>\n      <td>185777</td>\n      <td>Discharge summary</td>\n      <td>2191-03-23</td>\n      <td>4788</td>\n      <td>48</td>\n      <td>Admission Date:  [**2191-3-16**]     Discharge...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6</td>\n      <td>2109-06-21</td>\n      <td>F</td>\n      <td>107064</td>\n      <td>Discharge summary</td>\n      <td>2175-06-15</td>\n      <td>20825</td>\n      <td>66</td>\n      <td>Admission Date: [**2175-5-30**]        Dischar...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>2108-01-26</td>\n      <td>M</td>\n      <td>150750</td>\n      <td>Discharge summary</td>\n      <td>2149-11-14</td>\n      <td>57115</td>\n      <td>42</td>\n      <td>Name:  [**Known lastname 10050**], [**Known fi...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9</td>\n      <td>2108-01-26</td>\n      <td>M</td>\n      <td>150750</td>\n      <td>Discharge summary</td>\n      <td>2149-11-13</td>\n      <td>20070</td>\n      <td>42</td>\n      <td>Admission Date:  [**2149-11-9**]       Dischar...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>11</td>\n      <td>2128-02-22</td>\n      <td>F</td>\n      <td>194540</td>\n      <td>Discharge summary</td>\n      <td>2178-05-11</td>\n      <td>30120</td>\n      <td>50</td>\n      <td>Admission Date:  [**2178-4-16**]              ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>12</td>\n      <td>2032-03-24</td>\n      <td>M</td>\n      <td>112213</td>\n      <td>Discharge summary</td>\n      <td>2104-08-20</td>\n      <td>50972</td>\n      <td>72</td>\n      <td>Admission Date:  [**2104-8-7**]     Discharge ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>13</td>\n      <td>2127-02-27</td>\n      <td>F</td>\n      <td>143045</td>\n      <td>Discharge summary</td>\n      <td>2167-01-15</td>\n      <td>20168</td>\n      <td>40</td>\n      <td>Admission Date:  [**2167-1-8**]       Discharg...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>13</td>\n      <td>2127-02-27</td>\n      <td>F</td>\n      <td>143045</td>\n      <td>Discharge summary</td>\n      <td>2167-01-15</td>\n      <td>57099</td>\n      <td>40</td>\n      <td>Name:  [**Known lastname 9900**], [**Known fir...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>17</td>\n      <td>2087-07-14</td>\n      <td>F</td>\n      <td>161087</td>\n      <td>Discharge summary</td>\n      <td>2135-05-13</td>\n      <td>51783</td>\n      <td>48</td>\n      <td>Admission Date:  [**2135-5-9**]              D...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>17</td>\n      <td>2087-07-14</td>\n      <td>F</td>\n      <td>194023</td>\n      <td>Discharge summary</td>\n      <td>2134-12-31</td>\n      <td>51782</td>\n      <td>47</td>\n      <td>Admission Date:  [**2134-12-27**]             ...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>18</td>\n      <td>2116-11-29</td>\n      <td>M</td>\n      <td>188822</td>\n      <td>Discharge summary</td>\n      <td>2167-10-04</td>\n      <td>7824</td>\n      <td>51</td>\n      <td>Admission Date:  [**2167-10-2**]              ...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>18</td>\n      <td>2116-11-29</td>\n      <td>M</td>\n      <td>188822</td>\n      <td>Discharge summary</td>\n      <td>2167-10-02</td>\n      <td>55875</td>\n      <td>51</td>\n      <td>Name:  [**Known lastname 4919**] JR,[**Known f...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>18</td>\n      <td>2116-11-29</td>\n      <td>M</td>\n      <td>188822</td>\n      <td>Discharge summary</td>\n      <td>2167-10-02</td>\n      <td>7823</td>\n      <td>51</td>\n      <td>Admission Date:  [**2167-9-30**]              ...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>19</td>\n      <td>1808-08-05</td>\n      <td>M</td>\n      <td>109235</td>\n      <td>Discharge summary</td>\n      <td>2108-08-11</td>\n      <td>25231</td>\n      <td>300</td>\n      <td>Admission Date:  [**2108-8-5**]              D...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>20</td>\n      <td>2107-06-13</td>\n      <td>F</td>\n      <td>157681</td>\n      <td>Discharge summary</td>\n      <td>2183-05-03</td>\n      <td>17861</td>\n      <td>76</td>\n      <td>Admission Date:  [**2183-4-28**]       Dischar...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>20</td>\n      <td>2107-06-13</td>\n      <td>F</td>\n      <td>157681</td>\n      <td>Discharge summary</td>\n      <td>2183-04-28</td>\n      <td>17860</td>\n      <td>76</td>\n      <td>Admission Date:  [**2183-5-10**]       Dischar...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>21</td>\n      <td>2047-04-04</td>\n      <td>M</td>\n      <td>111970</td>\n      <td>Discharge summary</td>\n      <td>2135-02-08</td>\n      <td>7238</td>\n      <td>88</td>\n      <td>Admission Date:  [**2135-1-30**]              ...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>21</td>\n      <td>2047-04-04</td>\n      <td>M</td>\n      <td>109451</td>\n      <td>Discharge summary</td>\n      <td>2134-09-24</td>\n      <td>7237</td>\n      <td>87</td>\n      <td>Admission Date:  [**2134-9-11**]              ...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>22</td>\n      <td>2131-05-07</td>\n      <td>F</td>\n      <td>165315</td>\n      <td>Discharge summary</td>\n      <td>2196-04-10</td>\n      <td>12144</td>\n      <td>65</td>\n      <td>Admission Date:  [**2196-4-9**]       Discharg...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age_at_noteevent'] = pd.to_numeric(df['age_at_noteevent'], downcast='integer')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:18:57.530888765Z",
     "start_time": "2024-02-15T10:18:57.514317253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(55404, 9)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "55404 'adult' (15 or over) discharge summaries - this is what we expect from our exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following punctuation marks frequently appear in the middle of words or between words without spacing meaning they are missed by the tokenizer. What we need to is to split the tokens on these punctuation marks after we have tokenized. We do this with regex. We then retokenize. This will substantially decreases the number of our unique words which we will replace with <UNK>\n",
    "\n",
    "- ampersand\n",
    "- brackets\n",
    "- colons\n",
    "- forward slashes(make sure to leave dates alone though)\n",
    "- full stops\n",
    "- hyphens\n",
    "- equals signs\n",
    "- semicolons\n",
    "- plus signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T13:30:15.083772351Z",
     "start_time": "2024-02-15T13:30:14.969907469Z"
    }
   },
   "outputs": [],
   "source": [
    "date_regex = re.compile(r'([0-9])-([0-9][0-9]?)-([0-9])') # change date format so spacy can recognise\n",
    "newline_regex = re.compile(r'(\\\\n){3,}') # cap number of consecutive newline characters to 2\n",
    "newline_regex2 = re.compile(r'(\\\\r){3,}') # cap number of consecutive newline characters to 2\n",
    "ellipsis_regex = re.compile(r'(\\.){2,}')\n",
    "tilda_mult_regex = re.compile(r'(~){2,}')\n",
    "atsign_mult_regex = re.compile(r'(@){2,}')\n",
    "\n",
    "bracket_regex = re.compile(r'(.)(\\()(.)')\n",
    "bracket_regex2 = re.compile(r'(.)(\\))(.)')\n",
    "slash_regex = re.compile(r'(.)(\\/)([^0-9])')\n",
    "slash_regex2 = re.compile(r'([^0-9])(\\/)(.)')\n",
    "equals_regex = re.compile(r'(.)(=)(.)')\n",
    "colon_regex = re.compile(r'(.)(:)(.)')\n",
    "sq_bracket_regex = re.compile(r'(.)(\\[)(.)')\n",
    "dash_regex = re.compile(r'(.)(-)(.)')\n",
    "dash_regex2 = re.compile(r'(-)([\\S])')\n",
    "plus_regex = re.compile(r'(.)(\\+)(.)')\n",
    "amp_regex = re.compile(r'(.)(&)(.)')\n",
    "star_regex = re.compile(r'(.)(\\*)(.)') \n",
    "comma_regex = re.compile(r'(.)(,)(.)')\n",
    "tilda_regex = re.compile(r'(.)(~)(.)')\n",
    "pipe_regex = re.compile(r'(.)(\\|)(.)')\n",
    "atsign_regex = re.compile(r'(.)(@)(.)')\n",
    "dot_regex = re.compile(r'([^.][^0-9])(\\.)([^0-9,][^.])')\n",
    "\n",
    "dot_regex2 = re.compile(r'([^0-9])(\\.)(.)')\n",
    "semicol_regex = re.compile(r'(.);(.)')\n",
    "caret_regex = re.compile(r'(.)\\^(.)')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E053] Could not read config.cfg from /home/tristan/.pyenv/versions/3.9.18/envs/tf2/lib/python3.9/site-packages/en_core_sci_md/en_core_sci_md-0.3.0/config.cfg",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m nlp \u001B[38;5;241m=\u001B[39m \u001B[43mspacy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43men_core_sci_md\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.9.18/envs/tf2/lib/python3.9/site-packages/spacy/__init__.py:50\u001B[0m, in \u001B[0;36mload\u001B[0;34m(name, vocab, disable, exclude, config)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\n\u001B[1;32m     30\u001B[0m     name: Union[\u001B[38;5;28mstr\u001B[39m, Path],\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     35\u001B[0m     config: Union[Dict[\u001B[38;5;28mstr\u001B[39m, Any], Config] \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mSimpleFrozenDict(),\n\u001B[1;32m     36\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Language:\n\u001B[1;32m     37\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \n\u001B[1;32m     39\u001B[0m \u001B[38;5;124;03m    name (str): Package name or model path.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.9.18/envs/tf2/lib/python3.9/site-packages/spacy/util.py:324\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(name, vocab, disable, exclude, config)\u001B[0m\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m get_lang_class(name\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblank:\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m))()\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_package(name):  \u001B[38;5;66;03m# installed as package\u001B[39;00m\n\u001B[0;32m--> 324\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mload_model_from_package\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m Path(name)\u001B[38;5;241m.\u001B[39mexists():  \u001B[38;5;66;03m# path to model data directory\u001B[39;00m\n\u001B[1;32m    326\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m load_model_from_path(Path(name), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.9.18/envs/tf2/lib/python3.9/site-packages/spacy/util.py:357\u001B[0m, in \u001B[0;36mload_model_from_package\u001B[0;34m(name, vocab, disable, exclude, config)\u001B[0m\n\u001B[1;32m    342\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Load a model from an installed package.\u001B[39;00m\n\u001B[1;32m    343\u001B[0m \n\u001B[1;32m    344\u001B[0m \u001B[38;5;124;03mname (str): The package name.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;124;03mRETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[1;32m    355\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    356\u001B[0m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m importlib\u001B[38;5;241m.\u001B[39mimport_module(name)\n\u001B[0;32m--> 357\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.9.18/envs/tf2/lib/python3.9/site-packages/en_core_sci_md/__init__.py:10\u001B[0m, in \u001B[0;36mload\u001B[0;34m(**overrides)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moverrides):\n\u001B[0;32m---> 10\u001B[0m     nlp \u001B[38;5;241m=\u001B[39m \u001B[43mload_model_from_init_py\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;18;43m__file__\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moverrides\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m nlp\n",
      "File \u001B[0;32m~/.pyenv/versions/3.9.18/envs/tf2/lib/python3.9/site-packages/spacy/util.py:517\u001B[0m, in \u001B[0;36mload_model_from_init_py\u001B[0;34m(init_file, vocab, disable, exclude, config)\u001B[0m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m model_path\u001B[38;5;241m.\u001B[39mexists():\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE052\u001B[38;5;241m.\u001B[39mformat(path\u001B[38;5;241m=\u001B[39mdata_path))\n\u001B[0;32m--> 517\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mload_model_from_path\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    518\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    519\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    520\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmeta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmeta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    521\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    522\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.9.18/envs/tf2/lib/python3.9/site-packages/spacy/util.py:391\u001B[0m, in \u001B[0;36mload_model_from_path\u001B[0;34m(model_path, meta, vocab, disable, exclude, config)\u001B[0m\n\u001B[1;32m    389\u001B[0m config_path \u001B[38;5;241m=\u001B[39m model_path \u001B[38;5;241m/\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig.cfg\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    390\u001B[0m overrides \u001B[38;5;241m=\u001B[39m dict_to_dot(config)\n\u001B[0;32m--> 391\u001B[0m config \u001B[38;5;241m=\u001B[39m \u001B[43mload_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moverrides\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moverrides\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    392\u001B[0m nlp \u001B[38;5;241m=\u001B[39m load_model_from_config(config, vocab\u001B[38;5;241m=\u001B[39mvocab, disable\u001B[38;5;241m=\u001B[39mdisable, exclude\u001B[38;5;241m=\u001B[39mexclude)\n\u001B[1;32m    393\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m nlp\u001B[38;5;241m.\u001B[39mfrom_disk(model_path, exclude\u001B[38;5;241m=\u001B[39mexclude, overrides\u001B[38;5;241m=\u001B[39moverrides)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.9.18/envs/tf2/lib/python3.9/site-packages/spacy/util.py:548\u001B[0m, in \u001B[0;36mload_config\u001B[0;34m(path, overrides, interpolate)\u001B[0m\n\u001B[1;32m    546\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    547\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m config_path \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m config_path\u001B[38;5;241m.\u001B[39mexists() \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m config_path\u001B[38;5;241m.\u001B[39mis_file():\n\u001B[0;32m--> 548\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE053\u001B[38;5;241m.\u001B[39mformat(path\u001B[38;5;241m=\u001B[39mconfig_path, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig.cfg\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m    549\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m config\u001B[38;5;241m.\u001B[39mfrom_disk(\n\u001B[1;32m    550\u001B[0m         config_path, overrides\u001B[38;5;241m=\u001B[39moverrides, interpolate\u001B[38;5;241m=\u001B[39minterpolate\n\u001B[1;32m    551\u001B[0m     )\n",
      "\u001B[0;31mOSError\u001B[0m: [E053] Could not read config.cfg from /home/tristan/.pyenv/versions/3.9.18/envs/tf2/lib/python3.9/site-packages/en_core_sci_md/en_core_sci_md-0.3.0/config.cfg"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_sci_md')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T13:30:17.130898476Z",
     "start_time": "2024-02-15T13:30:17.063859Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T12:53:09.421116511Z",
     "start_time": "2024-02-15T12:53:09.335430668Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E053] Could not read config.cfg from /home/tristan/.pyenv/versions/3.9.18/envs/tf2/lib/python3.9/site-packages/en_core_sci_md/en_core_sci_md-0.3.0/config.cfg",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m nlp \u001B[38;5;241m=\u001B[39m \u001B[43mspacy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43men_core_sci_md\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m nlp\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39madd_special_case(\u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<PAR>\u001B[39m\u001B[38;5;124m'\u001B[39m, [{ORTH: \u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<PAR>\u001B[39m\u001B[38;5;124m'\u001B[39m}])\n\u001B[1;32m      3\u001B[0m nlp\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39madd_special_case(\u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<UNK>\u001B[39m\u001B[38;5;124m'\u001B[39m, [{ORTH: \u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<UNK>\u001B[39m\u001B[38;5;124m'\u001B[39m}])\n",
      "File \u001B[0;32m~/.pyenv/versions/3.9.18/envs/tf2/lib/python3.9/site-packages/spacy/__init__.py:50\u001B[0m, in \u001B[0;36mload\u001B[0;34m(name, vocab, disable, exclude, config)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\n\u001B[1;32m     30\u001B[0m     name: Union[\u001B[38;5;28mstr\u001B[39m, Path],\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     35\u001B[0m     config: Union[Dict[\u001B[38;5;28mstr\u001B[39m, Any], Config] \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mSimpleFrozenDict(),\n\u001B[1;32m     36\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Language:\n\u001B[1;32m     37\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \n\u001B[1;32m     39\u001B[0m \u001B[38;5;124;03m    name (str): Package name or model path.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.9.18/envs/tf2/lib/python3.9/site-packages/spacy/util.py:324\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(name, vocab, disable, exclude, config)\u001B[0m\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m get_lang_class(name\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblank:\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m))()\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_package(name):  \u001B[38;5;66;03m# installed as package\u001B[39;00m\n\u001B[0;32m--> 324\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mload_model_from_package\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m Path(name)\u001B[38;5;241m.\u001B[39mexists():  \u001B[38;5;66;03m# path to model data directory\u001B[39;00m\n\u001B[1;32m    326\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m load_model_from_path(Path(name), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.9.18/envs/tf2/lib/python3.9/site-packages/spacy/util.py:357\u001B[0m, in \u001B[0;36mload_model_from_package\u001B[0;34m(name, vocab, disable, exclude, config)\u001B[0m\n\u001B[1;32m    342\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Load a model from an installed package.\u001B[39;00m\n\u001B[1;32m    343\u001B[0m \n\u001B[1;32m    344\u001B[0m \u001B[38;5;124;03mname (str): The package name.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;124;03mRETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[1;32m    355\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    356\u001B[0m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m importlib\u001B[38;5;241m.\u001B[39mimport_module(name)\n\u001B[0;32m--> 357\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.9.18/envs/tf2/lib/python3.9/site-packages/en_core_sci_md/__init__.py:10\u001B[0m, in \u001B[0;36mload\u001B[0;34m(**overrides)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moverrides):\n\u001B[0;32m---> 10\u001B[0m     nlp \u001B[38;5;241m=\u001B[39m \u001B[43mload_model_from_init_py\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;18;43m__file__\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moverrides\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m nlp\n",
      "File \u001B[0;32m~/.pyenv/versions/3.9.18/envs/tf2/lib/python3.9/site-packages/spacy/util.py:517\u001B[0m, in \u001B[0;36mload_model_from_init_py\u001B[0;34m(init_file, vocab, disable, exclude, config)\u001B[0m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m model_path\u001B[38;5;241m.\u001B[39mexists():\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE052\u001B[38;5;241m.\u001B[39mformat(path\u001B[38;5;241m=\u001B[39mdata_path))\n\u001B[0;32m--> 517\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mload_model_from_path\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    518\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    519\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    520\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmeta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmeta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    521\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    522\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.9.18/envs/tf2/lib/python3.9/site-packages/spacy/util.py:391\u001B[0m, in \u001B[0;36mload_model_from_path\u001B[0;34m(model_path, meta, vocab, disable, exclude, config)\u001B[0m\n\u001B[1;32m    389\u001B[0m config_path \u001B[38;5;241m=\u001B[39m model_path \u001B[38;5;241m/\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig.cfg\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    390\u001B[0m overrides \u001B[38;5;241m=\u001B[39m dict_to_dot(config)\n\u001B[0;32m--> 391\u001B[0m config \u001B[38;5;241m=\u001B[39m \u001B[43mload_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moverrides\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moverrides\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    392\u001B[0m nlp \u001B[38;5;241m=\u001B[39m load_model_from_config(config, vocab\u001B[38;5;241m=\u001B[39mvocab, disable\u001B[38;5;241m=\u001B[39mdisable, exclude\u001B[38;5;241m=\u001B[39mexclude)\n\u001B[1;32m    393\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m nlp\u001B[38;5;241m.\u001B[39mfrom_disk(model_path, exclude\u001B[38;5;241m=\u001B[39mexclude, overrides\u001B[38;5;241m=\u001B[39moverrides)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.9.18/envs/tf2/lib/python3.9/site-packages/spacy/util.py:548\u001B[0m, in \u001B[0;36mload_config\u001B[0;34m(path, overrides, interpolate)\u001B[0m\n\u001B[1;32m    546\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    547\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m config_path \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m config_path\u001B[38;5;241m.\u001B[39mexists() \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m config_path\u001B[38;5;241m.\u001B[39mis_file():\n\u001B[0;32m--> 548\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE053\u001B[38;5;241m.\u001B[39mformat(path\u001B[38;5;241m=\u001B[39mconfig_path, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig.cfg\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m    549\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m config\u001B[38;5;241m.\u001B[39mfrom_disk(\n\u001B[1;32m    550\u001B[0m         config_path, overrides\u001B[38;5;241m=\u001B[39moverrides, interpolate\u001B[38;5;241m=\u001B[39minterpolate\n\u001B[1;32m    551\u001B[0m     )\n",
      "\u001B[0;31mOSError\u001B[0m: [E053] Could not read config.cfg from /home/tristan/.pyenv/versions/3.9.18/envs/tf2/lib/python3.9/site-packages/en_core_sci_md/en_core_sci_md-0.3.0/config.cfg"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_sci_md')\n",
    "nlp.tokenizer.add_special_case(u'<PAR>', [{ORTH: u'<PAR>'}])\n",
    "nlp.tokenizer.add_special_case(u'<UNK>', [{ORTH: u'<UNK>'}])\n",
    "\n",
    "num_tokens = list()\n",
    "num_sents = list()\n",
    "\n",
    "i = 0\n",
    "\n",
    "def tokenise_text(text, counter):\n",
    "    global i\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    text = date_regex.sub(r'\\1/\\2/\\3',text)\n",
    "    text = newline_regex.sub(r' \\\\n\\\\n ',text)\n",
    "    text = newline_regex2.sub(r' \\\\n\\\\n ',text)\n",
    "    text = ellipsis_regex.sub(r'.',text)\n",
    "    text = tilda_mult_regex.sub(r'~',text)\n",
    "    text = atsign_mult_regex.sub(r'@',text)\n",
    "    \n",
    "    text = text.replace(\"[**\",\"[\").replace(\"**]\",\"]\")\n",
    "    \n",
    "    tokens = nlp.tokenizer(text)\n",
    "    tokenised_text = \"\"\n",
    "    \n",
    "    for token in tokens:\n",
    "        tokenised_text = tokenised_text + token.text + \" \"\n",
    "    \n",
    "    tokenised_text = tokenised_text.replace(\"\\n\",\" <PAR> \")\n",
    "    \n",
    "    tokenised_text = bracket_regex.sub(r'\\1 \\2 \\3',tokenised_text)\n",
    "    tokenised_text = bracket_regex2.sub(r'\\1 \\2 \\3',tokenised_text)\n",
    "    tokenised_text = slash_regex.sub(r'\\1 \\2 \\3',tokenised_text)\n",
    "    tokenised_text = slash_regex2.sub(r'\\1 \\2 \\3',tokenised_text)\n",
    "    tokenised_text = slash_regex.sub(r'\\1 \\2 \\3',tokenised_text)\n",
    "    tokenised_text = equals_regex.sub(r'\\1 \\2 \\3',tokenised_text)\n",
    "    tokenised_text = colon_regex.sub(r'\\1 \\2 \\3',tokenised_text)\n",
    "    tokenised_text = sq_bracket_regex.sub(r'\\1 \\2 \\3',tokenised_text)\n",
    "    tokenised_text = dash_regex.sub(r'\\1 \\2 \\3',tokenised_text)\n",
    "    tokenised_text = dash_regex.sub(r'\\1 \\2 \\3',tokenised_text) # dash twice because sometimes it appears twice\n",
    "    tokenised_text = dash_regex.sub(r'\\1 \\2 \\3',tokenised_text) # dash thrice because sometimes it appears thrice\n",
    "    tokenised_text = dash_regex2.sub(r'\\1 \\2',tokenised_text) # dash thrice because sometimes it appears thrice\n",
    "    tokenised_text = plus_regex.sub(r'\\1 \\2 \\3',tokenised_text)\n",
    "    tokenised_text = star_regex.sub(r'\\1 \\2 \\3',tokenised_text)\n",
    "    tokenised_text = amp_regex.sub(r'\\1 \\2 \\3',tokenised_text)\n",
    "    tokenised_text = comma_regex.sub(r'\\1 \\2 \\3',tokenised_text)\n",
    "    tokenised_text = dot_regex.sub(r'\\1 \\2 \\3',tokenised_text)\n",
    "    tokenised_text = atsign_regex.sub(r'\\1 \\2 \\3',tokenised_text)\n",
    "    tokenised_text = tilda_regex.sub(r'\\1 \\2 \\3',tokenised_text)\n",
    "    tokenised_text = pipe_regex.sub(r'\\1 \\2 \\3',tokenised_text)\n",
    "    tokenised_text = dot_regex2.sub(r'\\1 \\3',tokenised_text)\n",
    "    tokenised_text = semicol_regex.sub(r'\\1 \\2',tokenised_text)\n",
    "    tokenised_text = caret_regex.sub(r'\\1 \\2',tokenised_text)\n",
    "        \n",
    "    tokenised_text = ' '.join(tokenised_text.split())\n",
    "    \n",
    "    tokens = nlp.tokenizer(tokenised_text)\n",
    "    tokenised_text = \"\"\n",
    "    \n",
    "    for token in tokens:\n",
    "        tokenised_text = tokenised_text + token.text + \" \"\n",
    "    \n",
    "    counter.update(tokenised_text.split())\n",
    "    tokenized_text = nltk.sent_tokenize(text)\n",
    "\n",
    "    num_tokens.append(len(list(tokens)))\n",
    "    num_sents.append(len(tokenized_text))\n",
    "    \n",
    "    i += 1\n",
    "    if (i % 100) == 0:\n",
    "        print (i)\n",
    "    \n",
    "    return tokenised_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T19:28:57.046335266Z"
    }
   },
   "outputs": [],
   "source": [
    "word_freq = Counter()\n",
    "df[\"text\"] = df[\"text\"].apply(tokenise_text, args = (word_freq,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we isolate the tokens which appear 3 times or fewer. They are mostly misspellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T19:28:57.046477193Z"
    }
   },
   "outputs": [],
   "source": [
    "infreq_words = [word for word in word_freq.keys() if word_freq[word] <= 3 and word[0].isdigit() == False]\n",
    "print(len(infreq_words))\n",
    "sorted(infreq_words)[10000:10050]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try and see if we can correct the misspellings using the `pyspellchecker` library by using the Levenshtein Distance algorithm and comparing against a dictionary. We first add the words with >3 occurrence to our dictionary. This is because they include a lot of scientific/medical terms which might not already be there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T19:28:57.046622285Z"
    }
   },
   "outputs": [],
   "source": [
    "freq_words = [word for word in word_freq.keys() if word_freq[word] > 3]\n",
    "add_to_dictionary = \" \".join(freq_words)\n",
    "f=open(\"../data/mimic_dict.txt\", \"w+\")\n",
    "f.write(add_to_dictionary)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T19:28:57.046779391Z"
    }
   },
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "spell.distance = 1  # set the distance parameter to just 1 edit away - much quicker\n",
    "spell.word_frequency.load_text_file('../data/mimic_dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T19:28:57.046922799Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spell' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m misspelled \u001B[38;5;241m=\u001B[39m \u001B[43mspell\u001B[49m\u001B[38;5;241m.\u001B[39munknown(infreq_words)\n\u001B[1;32m      2\u001B[0m misspell_dict \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, word \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(misspelled):\n",
      "\u001B[0;31mNameError\u001B[0m: name 'spell' is not defined"
     ]
    }
   ],
   "source": [
    "misspelled = spell.unknown(infreq_words)\n",
    "misspell_dict = {}\n",
    "for i, word in enumerate(misspelled):\n",
    "    if (word != spell.correction(word)):\n",
    "        misspell_dict[word] = spell.correction(word)\n",
    "    if (i % 100 == 0):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T19:28:57.313516706Z",
     "start_time": "2024-01-15T19:28:57.086539243Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'misspell_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[43mmisspell_dict\u001B[49m))\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mlist\u001B[39m(misspell_dict\u001B[38;5;241m.\u001B[39mitems())[:\u001B[38;5;241m10\u001B[39m]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'misspell_dict' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(misspell_dict))\n",
    "list(misspell_dict.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have correct spellings for many words in our dictionary that occurred <= 3 times. Anything else will be marked as `UNK`. We will save these as text files to avoid having to run this computation again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T19:28:57.192257764Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'infreq_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m unk_words \u001B[38;5;241m=\u001B[39m [word \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m \u001B[43minfreq_words\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(misspell_dict\u001B[38;5;241m.\u001B[39mkeys())]\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(unk_words))\n\u001B[1;32m      3\u001B[0m unk_words[:\u001B[38;5;241m100\u001B[39m]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'infreq_words' is not defined"
     ]
    }
   ],
   "source": [
    "unk_words = [word for word in infreq_words if word not in list(misspell_dict.keys())]\n",
    "print(len(unk_words))\n",
    "unk_words[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T19:28:57.415985594Z",
     "start_time": "2024-01-15T19:28:57.359563572Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'misspell_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtreatement\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[43mmisspell_dict\u001B[49m\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28mprint\u001B[39m (misspell_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtreatement\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'misspell_dict' is not defined"
     ]
    }
   ],
   "source": [
    "if 'treatement' in misspell_dict.keys():\n",
    "    print (misspell_dict['treatement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T19:28:57.415741545Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mnp\u001B[49m\u001B[38;5;241m.\u001B[39msavetxt(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../data/discharge_unk_words.txt\u001B[39m\u001B[38;5;124m'\u001B[39m, unk_words, fmt\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m, newline\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39mlinesep)\n\u001B[1;32m      3\u001B[0m f\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../data/discharge_typos.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw+\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m misspell_dict:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.savetxt('../data/discharge_unk_words.txt', unk_words, fmt='%s', newline=os.linesep)\n",
    "\n",
    "f=open(\"../data/discharge_typos.txt\", \"w+\")\n",
    "for key in misspell_dict:\n",
    "    f.write(key + '\\t' + misspell_dict[key] + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T19:28:57.525361709Z",
     "start_time": "2024-01-15T19:28:57.465143224Z"
    }
   },
   "outputs": [],
   "source": [
    "# LOAD TYPOS AND UNKS FROM TXT FILE IF ALREADY GENERATED\n",
    "\n",
    "#discharge_typos = pd.read_csv('../data/disharge_typos.txt', sep='\\t',header=None)\n",
    "#misspell_dict = {row[0]: row[1] for row in discharge_typos.values}\n",
    "#unk_words = pd.read_csv('../data/discharge_unk_words.txt', sep='\\t', header=None)\n",
    "#unk_words = list(unk_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will correct spelling mistakes whilst any word left uncorrected will be replaced with `<UNK>`. Instead of using regex, we instead use `FlashText` which is considerably faster for replacing words from a large dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T19:28:57.556613386Z",
     "start_time": "2024-01-15T19:28:57.525762648Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'misspell_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m keywords \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[43mmisspell_dict\u001B[49m\u001B[38;5;241m.\u001B[39mkeys())\n\u001B[1;32m      2\u001B[0m clean \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(misspell_dict\u001B[38;5;241m.\u001B[39mvalues())\n\u001B[1;32m      4\u001B[0m processor \u001B[38;5;241m=\u001B[39m KeywordProcessor()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'misspell_dict' is not defined"
     ]
    }
   ],
   "source": [
    "keywords = list(misspell_dict.keys())\n",
    "clean = list(misspell_dict.values())\n",
    "\n",
    "processor = KeywordProcessor()\n",
    "\n",
    "for keyword_name, clean_name in zip(keywords, clean):\n",
    "    processor.add_keyword(keyword_name, clean_name)\n",
    "    \n",
    "for unk in unk_words:\n",
    "    processor.add_keyword(unk, \"<UNK>\")\n",
    "\n",
    "counter = 0\n",
    "def fix_typos(text):\n",
    "    global counter\n",
    "    \n",
    "    found = processor.replace_keywords(text)\n",
    "    \n",
    "    counter+=1\n",
    "    if (counter % 100) == 0:\n",
    "        print (counter)\n",
    "    \n",
    "    return found\n",
    "\n",
    "\n",
    "# apply tokenising function elementwise\n",
    "df[\"text\"] = df[\"text\"].apply(fix_typos)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T19:28:57.555583148Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[43mdf\u001B[49m\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m20\u001B[39m)[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "' '.join(df.head(20)['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text field has now been fully cleaned and tokenised. We can proceed to extract the first few tokens to use as a hint and move forth with joining other tables and partitioning the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T19:28:57.638301558Z",
     "start_time": "2024-01-15T19:28:57.599681972Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m         \u001B[38;5;28mprint\u001B[39m (counter)\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(l[:\u001B[38;5;241m10\u001B[39m]) \u001B[38;5;66;03m# first 10 tokens\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhint\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x: produce_hint(x))\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(df\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m     13\u001B[0m df\u001B[38;5;241m.\u001B[39mhead()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "def produce_hint(text):\n",
    "    global counter\n",
    "    l = text.split()\n",
    "    counter += 1\n",
    "    if (counter % 10000) == 0:\n",
    "        print (counter)\n",
    "    return ' '.join(l[:10]) # first 10 tokens\n",
    "\n",
    "df['hint'] = df['text'].map(lambda x: produce_hint(x))\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T19:28:57.614471693Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# patients above 89 years of age had their dob modified to be 300 years old at time of first event for privacy reasons\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# change their age to instead be 90\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241m.\u001B[39mloc[df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mage_at_noteevent\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m200\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mage_at_noteevent\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m90\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# patients above 89 years of age had their dob modified to be 300 years old at time of first event for privacy reasons\n",
    "# change their age to instead be 90\n",
    "\n",
    "df.loc[df['age_at_noteevent'] > 200, 'age_at_noteevent'] = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now merge our dataframe with some extra information from the admissions table. This preprocessing has already been done in another notebook so here we just load the csv file. Primarily, this table provides us with the ethnicity of the patient and whether or not they go on to have an unplanned readmission within 30 days of being discharged. This is relevant for one of our downstream tasks using the artificial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T19:28:57.720330703Z",
     "start_time": "2024-01-15T19:28:57.683124585Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# admissions data\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m admissions \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../data/df_adm.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m admissions\u001B[38;5;241m.\u001B[39mhead()\n\u001B[1;32m      5\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mmerge(df, admissions,  how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m'\u001B[39m, left_on\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubject_id\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhadm_id\u001B[39m\u001B[38;5;124m'\u001B[39m], right_on \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubject_id\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhadm_id\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# admissions data\n",
    "\n",
    "admissions = pd.read_csv('../data/df_adm.csv', sep=',')\n",
    "admissions.head()\n",
    "df = pd.merge(df, admissions,  how='left', left_on=['subject_id','hadm_id'], right_on = ['subject_id','hadm_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load all the context data from other tables. We'll be using this to construct the structured input to our encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T19:28:57.815827497Z",
     "start_time": "2024-01-15T19:28:57.725335207Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# lab items data\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m df_labitems \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mread_sql_query(\u001B[38;5;124m'''\u001B[39m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124m  SELECT l.subject_id, l.charttime, l.value, l.valueuom, l.flag, d.label\u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124m  FROM labevents l\u001B[39m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124m  INNER JOIN d_labitems d \u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124m  USING (itemid)\u001B[39m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124m  ORDER BY l.subject_id\u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124m  --LIMIT 10000;\u001B[39m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124m'''\u001B[39m, cnx)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(df_labitems\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m     13\u001B[0m df_labitems\u001B[38;5;241m.\u001B[39mhead()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# lab items data\n",
    "\n",
    "df_labitems = pd.read_sql_query('''\n",
    "  SELECT l.subject_id, l.charttime, l.value, l.valueuom, l.flag, d.label\n",
    "  FROM labevents l\n",
    "  INNER JOIN d_labitems d \n",
    "  USING (itemid)\n",
    "  ORDER BY l.subject_id\n",
    "  --LIMIT 10000;\n",
    "''', cnx)\n",
    "\n",
    "print(df_labitems.shape)\n",
    "df_labitems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T19:28:57.789693232Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# prescriptions data\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m df_prescriptions \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mread_sql_query(\u001B[38;5;124m'''\u001B[39m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124m  SELECT subject_id, startdate, enddate, LOWER(drug), prod_strength\u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124m  FROM prescriptions\u001B[39m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124m  ORDER BY subject_id\u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124m  --LIMIT 10000;\u001B[39m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124m'''\u001B[39m, cnx)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(df_prescriptions\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m     11\u001B[0m df_prescriptions\u001B[38;5;241m.\u001B[39mhead()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# prescriptions data\n",
    "\n",
    "df_prescriptions = pd.read_sql_query('''\n",
    "  SELECT subject_id, startdate, enddate, LOWER(drug), prod_strength\n",
    "  FROM prescriptions\n",
    "  ORDER BY subject_id\n",
    "  --LIMIT 10000;\n",
    "''', cnx)\n",
    "\n",
    "print(df_prescriptions.shape)\n",
    "df_prescriptions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T19:28:57.887281913Z",
     "start_time": "2024-01-15T19:28:57.863061095Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# diagnoses data\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m df_diagnoses \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mread_sql_query(\u001B[38;5;124m'''\u001B[39m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124m  SELECT d.subject_id, d.hadm_id, d.seq_num, d.icd9_code, icd.short_title, LOWER(icd.long_title)\u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124m  FROM diagnoses_icd d\u001B[39m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124m  INNER JOIN d_icd_diagnoses icd \u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124m  USING (icd9_code)\u001B[39m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124m  ORDER BY d.subject_id, d.seq_num\u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124m  --LIMIT 10000;\u001B[39m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124m'''\u001B[39m, cnx)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(df_diagnoses\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m     13\u001B[0m df_diagnoses\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m10\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# diagnoses data\n",
    "\n",
    "df_diagnoses = pd.read_sql_query('''\n",
    "  SELECT d.subject_id, d.hadm_id, d.seq_num, d.icd9_code, icd.short_title, LOWER(icd.long_title)\n",
    "  FROM diagnoses_icd d\n",
    "  INNER JOIN d_icd_diagnoses icd \n",
    "  USING (icd9_code)\n",
    "  ORDER BY d.subject_id, d.seq_num\n",
    "  --LIMIT 10000;\n",
    "''', cnx)\n",
    "\n",
    "print(df_diagnoses.shape)\n",
    "df_diagnoses.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T19:28:57.967647424Z",
     "start_time": "2024-01-15T19:28:57.894592777Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# procedures data\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m hadm_proc_subset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../data/df_proc_hadm_ids.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhadm_id\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      5\u001B[0m df_procedures \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_sql_query(\u001B[38;5;124m'''\u001B[39m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124m  SELECT p.subject_id, p.hadm_id, p.seq_num, p.icd9_code, icd.short_title, LOWER(icd.long_title)\u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124m  FROM procedures_icd p\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124m  --LIMIT 10000;\u001B[39m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124m'''\u001B[39m, cnx)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(df_procedures\u001B[38;5;241m.\u001B[39mshape)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# procedures data\n",
    "\n",
    "hadm_proc_subset = list(pd.read_csv('../data/df_proc_hadm_ids.csv', sep=',')['hadm_id'])\n",
    "\n",
    "df_procedures = pd.read_sql_query('''\n",
    "  SELECT p.subject_id, p.hadm_id, p.seq_num, p.icd9_code, icd.short_title, LOWER(icd.long_title)\n",
    "  FROM procedures_icd p\n",
    "  INNER JOIN d_icd_procedures icd \n",
    "  USING (icd9_code)\n",
    "  ORDER BY p.subject_id, p.seq_num DESC\n",
    "  --LIMIT 10000;\n",
    "''', cnx)\n",
    "\n",
    "print(df_procedures.shape)\n",
    "df_procedures = df_procedures[df_procedures['hadm_id'].isin(hadm_proc_subset)]\n",
    "print(df_procedures.shape)\n",
    "df_procedures.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T19:28:58.020154895Z",
     "start_time": "2024-01-15T19:28:57.967997071Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# microbiology data\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m df_microbiology \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mread_sql_query(\u001B[38;5;124m'''\u001B[39m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124m  SELECT \u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124m      subject_id, \u001B[39m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124m      hadm_id, \u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124m      MAX(chartdate) AS chartdate, \u001B[39m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124m      LOWER(spec_type_desc), \u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124m      string_agg(DISTINCT(LOWER(org_name)), \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m) AS organism\u001B[39m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124m  \u001B[39m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124m  FROM microbiologyevents\u001B[39m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124m  GROUP BY subject_id, hadm_id, charttime, spec_type_desc\u001B[39m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;124m  ORDER BY subject_id, chartdate DESC\u001B[39m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;124m  --LIMIT 10000;\u001B[39m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124m'''\u001B[39m, cnx)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(df_microbiology\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m     18\u001B[0m df_microbiology\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m20\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# microbiology data\n",
    "\n",
    "df_microbiology = pd.read_sql_query('''\n",
    "  SELECT \n",
    "      subject_id, \n",
    "      hadm_id, \n",
    "      MAX(chartdate) AS chartdate, \n",
    "      LOWER(spec_type_desc), \n",
    "      string_agg(DISTINCT(LOWER(org_name)), ', ') AS organism\n",
    "  \n",
    "  FROM microbiologyevents\n",
    "  GROUP BY subject_id, hadm_id, charttime, spec_type_desc\n",
    "  ORDER BY subject_id, chartdate DESC\n",
    "  --LIMIT 10000;\n",
    "''', cnx)\n",
    "\n",
    "print(df_microbiology.shape)\n",
    "df_microbiology.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T19:28:58.009703984Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# phenotype classification subject - we need to make sure these are part of our test set\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m annotations \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../data/annotations.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m annotations\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhadm_id\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      5\u001B[0m annotations\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubject_id\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# phenotype classification subject - we need to make sure these are part of our test set\n",
    "\n",
    "annotations = pd.read_csv('../data/annotations.csv')\n",
    "annotations.columns.values[0] = 'hadm_id'\n",
    "annotations.columns.values[1] = 'subject_id'\n",
    "\n",
    "pheno_subjects = list(annotations['subject_id'])\n",
    "print(annotations.shape)\n",
    "annotations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the dataset into training, validation and test sets. As we do so, we query the relevant context data and append it to the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T19:28:58.095859848Z",
     "start_time": "2024-01-15T19:28:58.020808123Z"
    }
   },
   "outputs": [],
   "source": [
    "##%%timeit -n 3 -r 3\n",
    "\n",
    "# Split the dataset in a grouped and stratified manner\n",
    "\n",
    "def StratifiedGroupShuffleSplit(df_main):\n",
    "    \n",
    "    np.random.seed(768) # seeded for reproducibility\n",
    "    df_main = df_main.reindex(np.random.permutation(df_main.index)) # shuffle dataset\n",
    "    \n",
    "    # create empty train, val and test datasets\n",
    "    df_train = pd.DataFrame()\n",
    "    df_val = pd.DataFrame()\n",
    "    df_test = pd.DataFrame()\n",
    "\n",
    "    hparam_mse_wgt = 0.8 # must be between 0 and 1\n",
    "    assert(0 <= hparam_mse_wgt <= 1)\n",
    "    train_proportion = 0.8 # must be between 0 and 1\n",
    "    assert(0 <= train_proportion <= 1)\n",
    "    val_test_proportion = (1-train_proportion)/2\n",
    "\n",
    "    subject_grouped_df_main = df_main.groupby(['subject_id'], sort=False, as_index=False)\n",
    "    readmit_grouped_df_main = df_main.groupby('30d_unplan_readmit').count()[['subject_id']]/len(df_main)*100 \n",
    "    \n",
    "    # function to calculate loss\n",
    "    def calc_mse_loss(df):\n",
    "        grouped_df = df.groupby('30d_unplan_readmit').count()[['subject_id']]/len(df)*100\n",
    "        df_temp = readmit_grouped_df_main.join(grouped_df, on = '30d_unplan_readmit', how = 'left', lsuffix = '_main')\n",
    "        df_temp.fillna(0, inplace=True)\n",
    "        df_temp['diff'] = (df_temp['subject_id_main'] - df_temp['subject_id'])**2\n",
    "        mse_loss = np.mean(df_temp['diff'])\n",
    "        return mse_loss\n",
    "    \n",
    "    directory = \"../data/preprocessed/\"\n",
    "    \n",
    "    f_train = open(directory + \"src-train.txt\",\"w+\")\n",
    "    f_val = open(directory + \"src-val.txt\",\"w+\")\n",
    "    f_test = open(directory + \"src-test.txt\",\"w+\")\n",
    "    \n",
    "    len_train = 0\n",
    "    len_val = 0\n",
    "    len_test = 0\n",
    "    total_records = 0\n",
    "    i = 0\n",
    "\n",
    "    # loop the groups of subjects one by one\n",
    "    for _, group in subject_grouped_df_main:\n",
    "\n",
    "        total_records = len_train + len_val + len_test\n",
    "        g = pd.DataFrame(group)\n",
    "        subject_id = g['subject_id'].iloc[0]\n",
    "        \n",
    "        pre_left = df_prescriptions['subject_id'].searchsorted(subject_id, 'left')\n",
    "        pre_right = df_prescriptions['subject_id'].searchsorted(subject_id, 'right')\n",
    "        \n",
    "        lab_left = df_labitems['subject_id'].searchsorted(subject_id, 'left')\n",
    "        lab_right = df_labitems['subject_id'].searchsorted(subject_id, 'right')\n",
    "        \n",
    "        diag_left = df_diagnoses['subject_id'].searchsorted(subject_id, 'left')\n",
    "        diag_right = df_diagnoses['subject_id'].searchsorted(subject_id, 'right')\n",
    "        \n",
    "        proc_left = df_procedures['subject_id'].searchsorted(subject_id, 'left')\n",
    "        proc_right = df_procedures['subject_id'].searchsorted(subject_id, 'right')\n",
    "        \n",
    "        micro_left = df_microbiology['subject_id'].searchsorted(subject_id, 'left')\n",
    "        micro_right = df_microbiology['subject_id'].searchsorted(subject_id, 'right')\n",
    "        \n",
    "        g_prescriptions = df_prescriptions[pre_left:pre_right]\n",
    "        g_labitems = df_labitems[lab_left:lab_right]\n",
    "        g_diagnoses = df_diagnoses[diag_left:diag_right]\n",
    "        g_procedures = df_procedures[proc_left:proc_right]\n",
    "        g_microbiology = df_microbiology[micro_left:micro_right]\n",
    "        \n",
    "        train = False\n",
    "        val = False\n",
    "        test = False\n",
    "        i += 1\n",
    "        \n",
    "        # all subjects in the phenotyping dataset need to be in the test set\n",
    "        if subject_id in pheno_subjects:\n",
    "            df_test = df_test.append(g, ignore_index=True)\n",
    "            len_test += len(g)\n",
    "            test = True\n",
    "            \n",
    "        # just to add something to each group to start off with - otherwise we end up dividing by 0\n",
    "        elif (len_train == 0 or len_val == 0 or len_test == 0):\n",
    "            if (len_train == 0):\n",
    "                df_train = df_train.append(g, ignore_index=True)\n",
    "                len_train += len(g)\n",
    "                train = True\n",
    "            elif (len_val == 0):\n",
    "                df_val = df_val.append(g, ignore_index=True)\n",
    "                len_val += len(g)\n",
    "                val = True\n",
    "            else:\n",
    "                df_test = df_test.append(g, ignore_index=True)\n",
    "                len_test += len(g)\n",
    "                test = True\n",
    "        \n",
    "        # every 3rd group, balance the groups jointly by proportion and by ratio of unplanned readmissions\n",
    "        elif (i % 3 == 0):\n",
    "            \n",
    "            mse_loss_diff_train = calc_mse_loss(df_train) - calc_mse_loss(df_train.append(g, ignore_index=True))\n",
    "            mse_loss_diff_val = calc_mse_loss(df_val) - calc_mse_loss(df_val.append(g, ignore_index=True))\n",
    "            mse_loss_diff_test = calc_mse_loss(df_test) - calc_mse_loss(df_test.append(g, ignore_index=True))\n",
    "\n",
    "            len_diff_train = (train_proportion - (len_train/total_records)) * 100\n",
    "            len_diff_val = (val_test_proportion - (len_val/total_records)) * 100\n",
    "            len_diff_test = (val_test_proportion - (len_test/total_records)) * 100\n",
    "\n",
    "            len_loss_diff_train = len_diff_train * abs(len_diff_train)\n",
    "            len_loss_diff_val = len_diff_val * abs(len_diff_val)\n",
    "            len_loss_diff_test = len_diff_test * abs(len_diff_test)\n",
    "\n",
    "            loss_train = (hparam_mse_wgt * mse_loss_diff_train) + ((1-hparam_mse_wgt) * len_loss_diff_train)\n",
    "            loss_val = (hparam_mse_wgt * mse_loss_diff_val) + ((1-hparam_mse_wgt) * len_loss_diff_val)\n",
    "            loss_test = (hparam_mse_wgt * mse_loss_diff_test) + ((1-hparam_mse_wgt) * len_loss_diff_test)\n",
    "\n",
    "            if (max(loss_train,loss_val,loss_test) == loss_train):\n",
    "                df_train = df_train.append(g, ignore_index=True)\n",
    "                len_train += len(g)\n",
    "                train = True\n",
    "            elif (max(loss_train,loss_val,loss_test) == loss_val):\n",
    "                df_val = df_val.append(g, ignore_index=True)\n",
    "                len_val += len(g)\n",
    "                val = True\n",
    "            else:\n",
    "                df_test = df_test.append(g, ignore_index=True)\n",
    "                len_test += len(g)\n",
    "                test = True\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print (\"Group \" + str(i) + \": loss_train: \" + str(loss_train) + \" | \" + \n",
    "                                             \"loss_val: \" + str(loss_val) + \" | \" + \n",
    "                                             \"loss_test: \" + str(loss_test))\n",
    "\n",
    "        # all the other groups - divided simply by ratios of the dataset splits\n",
    "        else:\n",
    "            train_diff = train_proportion - (len_train/total_records)\n",
    "            val_diff = val_test_proportion - (len_val/total_records)\n",
    "            test_diff = val_test_proportion - (len_test/total_records)\n",
    "            \n",
    "            if (max(train_diff, val_diff, test_diff) == train_diff):\n",
    "                df_train = df_train.append(g, ignore_index=True)\n",
    "                len_train += len(g)\n",
    "                train = True\n",
    "            elif (max(train_diff, val_diff, test_diff) == val_diff):\n",
    "                df_val = df_val.append(g, ignore_index=True)\n",
    "                len_val += len(g)\n",
    "                val = True\n",
    "            else:\n",
    "                df_test = df_test.append(g, ignore_index=True)\n",
    "                len_test += len(g)\n",
    "                test = True\n",
    "        \n",
    "        # loop through every row in the group to get relevant prescriptions and lab items before appending to file\n",
    "        for j, row in enumerate(g.itertuples()):\n",
    "            \n",
    "            hadm_id = row[4]\n",
    "            # context is all day the day before the discharge day and the day of the discharge\n",
    "            chartdate = datetime.combine(row[6], datetime.min.time()) + timedelta(days=1)\n",
    "            cutoff = chartdate - timedelta(days=2)\n",
    "            micro_cutoff = chartdate - timedelta(days=4) # greater context window for microbiology results\n",
    "            \n",
    "            ###### LAB RESULTS\n",
    "            lab_condition = np.logical_and((g_labitems.charttime >= cutoff),\n",
    "                                           (g_labitems.charttime < chartdate))\n",
    "            lab_items = g_labitems[lab_condition]\n",
    "            lab_items = lab_items.sort_values(by=['charttime'], ascending=False)\n",
    "            \n",
    "            ###### PRESCRIPTIONS\n",
    "            pre_condition = np.logical_and((g_prescriptions.startdate >= cutoff),\n",
    "                                           (g_prescriptions.startdate < chartdate))\n",
    "            prescriptions = g_prescriptions[pre_condition]\n",
    "            prescriptions = prescriptions.sort_values(by=['startdate'], ascending=False)\n",
    "            \n",
    "            ###### DIAGNOSES\n",
    "            diagnoses = g_diagnoses[g_diagnoses.hadm_id == hadm_id]\n",
    "            diagnoses = diagnoses.sort_values(by=['seq_num'], ascending=True)\n",
    "            \n",
    "            ###### PROCEDURES\n",
    "            if j == 0:\n",
    "                procedures = g_procedures[g_procedures.hadm_id == hadm_id]\n",
    "                procedures = procedures.sort_values(by=['seq_num'], ascending=False)\n",
    "            \n",
    "            ###### MICROBIOLOGY TESTS\n",
    "            micro_condition = np.logical_and((g_microbiology.chartdate >= micro_cutoff),\n",
    "                                             (g_microbiology.chartdate < chartdate))\n",
    "            microbiology = g_microbiology[micro_condition]\n",
    "            microbiology = microbiology.sort_values(by=['chartdate'], ascending=False)\n",
    "            \n",
    "            ##########################################\n",
    "            lab_items_list = \"\"\n",
    "            lab_items_length = len(lab_items)\n",
    "            if (lab_items_length > 0):\n",
    "                for k, lab_row in enumerate(lab_items.itertuples()):\n",
    "                    flag = \"\"\n",
    "                    if (pd.isna(lab_row[5]) == False):\n",
    "                        flag = \" , \" + str(lab_row[5])\n",
    "\n",
    "                    lab_items_list += str(lab_row[6]) + \" , \" + str(lab_row[3]) + \" , \" + str(lab_row[4]) + flag\n",
    "                    if (k != (lab_items_length - 1)):\n",
    "                        lab_items_list += \" | \"\n",
    "            \n",
    "            ##########################################\n",
    "            prescriptions_list = \"\"\n",
    "            prescriptions_length = len(prescriptions)\n",
    "            if (prescriptions_length > 0):\n",
    "                for k, pre_row in enumerate(prescriptions.itertuples()):\n",
    "                    prescriptions_list += str(pre_row[4]) + \" , \" + str(pre_row[5])\n",
    "                    if (k != (prescriptions_length - 1)):\n",
    "                        prescriptions_list += \" | \"\n",
    "\n",
    "            ##########################################\n",
    "            diagnoses_list = \"\"\n",
    "            diagnoses_length = len(diagnoses)\n",
    "            if (diagnoses_length > 0):\n",
    "                for k, diag_row in enumerate(diagnoses.itertuples()):\n",
    "                    diagnoses_list += str(diag_row[6])\n",
    "                    if (k != (diagnoses_length - 1)):\n",
    "                        diagnoses_list += \" | \"\n",
    "\n",
    "            ##########################################\n",
    "            procedures_list = \"\"\n",
    "            if j == 0: # only allow procedures for the most recent hospital admission for a given subject\n",
    "                procedures_length = len(procedures)\n",
    "                if (procedures_length > 0):\n",
    "                    for k, proc_row in enumerate(procedures.itertuples()):\n",
    "                        procedures_list += str(proc_row[6])\n",
    "                        if (k != (procedures_length - 1)):\n",
    "                            procedures_list += \" | \"\n",
    "\n",
    "            ##########################################\n",
    "            microbiology_list = \"\"\n",
    "            microbiology_length = len(microbiology)\n",
    "            if (microbiology_length > 0):\n",
    "                for k, micro_row in enumerate(microbiology.itertuples()):\n",
    "                    microbiology_list += str(micro_row[4]) + \" : \" + str(micro_row[5])\n",
    "                    if (k != (microbiology_length - 1)):\n",
    "                        microbiology_list += \" | \"\n",
    "                        \n",
    "            ##########################################\n",
    "            \n",
    "            if (train == True):\n",
    "                f_train.write(str(row[10]) + \" <H> \" + str(row[3]) + \" <G> \" + str(row[8]) + \" <A> \" + \n",
    "                    str(row[11]) + \" <E> \" + diagnoses_list + \" <D> \" + procedures_list + \" <P> \" +\n",
    "                    prescriptions_list + \" <M> \" + microbiology_list + \" <T> \" + lab_items_list + \" <L>\" + \"\\n\")\n",
    "            elif (val == True):\n",
    "                f_val.write(str(row[10]) + \" <H> \" + str(row[3]) + \" <G> \" + str(row[8]) + \" <A> \" + \n",
    "                    str(row[11]) + \" <E> \" + diagnoses_list + \" <D> \" + procedures_list + \" <P> \" +\n",
    "                    prescriptions_list + \" <M> \" + microbiology_list + \" <T> \" + lab_items_list + \" <L>\" + \"\\n\")\n",
    "            else:\n",
    "                f_test.write(str(row[10]) + \" <H> \" + str(row[3]) + \" <G> \" + str(row[8]) + \" <A> \" + \n",
    "                    str(row[11]) + \" <E> \" + diagnoses_list + \" <D> \" + procedures_list + \" <P> \" +\n",
    "                    prescriptions_list + \" <M> \" + microbiology_list + \" <T> \" + lab_items_list + \" <L>\" + \"\\n\")\n",
    "\n",
    "    f_train.close()\n",
    "    f_val.close()\n",
    "    f_test.close()\n",
    "    \n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T19:28:58.163505782Z",
     "start_time": "2024-01-15T19:28:58.083555384Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m src_train, src_val, src_test \u001B[38;5;241m=\u001B[39m StratifiedGroupShuffleSplit(\u001B[43mdf\u001B[49m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "src_train, src_val, src_test = StratifiedGroupShuffleSplit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have finally split the dataset into training, evaluation and test datasets, let's just inspect the stratification to ensure that we have successfully managed to have a uniform ratio of unplanned readmissions across all three datasets. And that they have been split correctly in the ratio 8:1:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T19:28:58.149721527Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[43msrc_train\u001B[49m), \u001B[38;5;28mlen\u001B[39m(src_val), \u001B[38;5;28mlen\u001B[39m(src_test))\n\u001B[1;32m      3\u001B[0m readmit_grouped_df_main \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mgroupby(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m30d_unplan_readmit\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mcount()[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubject_id\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(df)\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\n\u001B[1;32m      4\u001B[0m grouped_train \u001B[38;5;241m=\u001B[39m src_train\u001B[38;5;241m.\u001B[39mgroupby(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m30d_unplan_readmit\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mcount()[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubject_id\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(src_train)\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'src_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(src_train), len(src_val), len(src_test))\n",
    "\n",
    "readmit_grouped_df_main = df.groupby('30d_unplan_readmit').count()[['subject_id']]/len(df)*100\n",
    "grouped_train = src_train.groupby('30d_unplan_readmit').count()[['subject_id']]/len(src_train)*100\n",
    "grouped_val = src_val.groupby('30d_unplan_readmit').count()[['subject_id']]/len(src_val)*100\n",
    "grouped_test = src_test.groupby('30d_unplan_readmit').count()[['subject_id']]/len(src_test)*100\n",
    "\n",
    "df_summary = readmit_grouped_df_main.join(grouped_train, on = '30d_unplan_readmit', how = 'left', rsuffix = '_train')\n",
    "df_summary = df_summary.join(grouped_val, on = '30d_unplan_readmit', how = 'left', rsuffix = '_val')\n",
    "df_summary = df_summary.join(grouped_test, on = '30d_unplan_readmit', how = 'left', rsuffix = '_test')\n",
    "df_summary.fillna(0, inplace=True)\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio of 8:1:1 is roughly correct. The ratio of unplanned readmissions is also not too bad. However, the test dataset seems to contain considerably more unplanned readmissions: 13% vs 6%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T19:28:58.266560586Z",
     "start_time": "2024-01-15T19:28:58.166317430Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m# of Discharge summaries for subjects in phenotyping dataset: \u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mlen\u001B[39m(\u001B[43msrc_test\u001B[49m[src_test[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubject_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39misin(pheno_subjects)]))\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m# of records in the phenotyping dataset\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;28mlen\u001B[39m(pheno_subjects))\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m# of unique subjects in the phenotyping dataset\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;28mlen\u001B[39m(annotations\u001B[38;5;241m.\u001B[39msubject_id\u001B[38;5;241m.\u001B[39munique()))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'src_test' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"# of Discharge summaries for subjects in phenotyping dataset: \", len(src_test[src_test['subject_id'].isin(pheno_subjects)]))\n",
    "print(\"# of records in the phenotyping dataset\",len(pheno_subjects))\n",
    "print(\"# of unique subjects in the phenotyping dataset\",len(annotations.subject_id.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On closer inspection, this appears to be due to the fact that we require all subjects in the phenotyping dataset to be in the test set. Despite comprising only 1045 unique subjects, these subjects are responsible for 3672 discharge summaries - much higher than average. This naturally also corresponds to a much greater incidence of 30d unplanned readmission, hence the greater incidence for this in the test dataset. The difference between 13% and 6% is small enough that we will tolerate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have saved our source datasets, now let's now save our target and reference datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T19:28:58.229732664Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# create target files\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m tgt_train \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mDataFrame(src_train, columns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m      4\u001B[0m tgt_val \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(src_val, columns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m      5\u001B[0m tgt_test \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(src_test, columns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# create target files\n",
    "\n",
    "tgt_train = pd.DataFrame(src_train, columns = [\"text\"])\n",
    "tgt_val = pd.DataFrame(src_val, columns = [\"text\"])\n",
    "tgt_test = pd.DataFrame(src_test, columns = [\"text\"])\n",
    "\n",
    "print(tgt_test.shape)\n",
    "tgt_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T19:28:58.372407316Z",
     "start_time": "2024-01-15T19:28:58.273866390Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# save target files to disk\u001B[39;00m\n\u001B[1;32m      3\u001B[0m directory \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../data/preprocessed/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 5\u001B[0m \u001B[43mnp\u001B[49m\u001B[38;5;241m.\u001B[39msavetxt(directory \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtgt-train.txt\u001B[39m\u001B[38;5;124m'\u001B[39m, tgt_train, fmt\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m, newline\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39mlinesep)\n\u001B[1;32m      6\u001B[0m np\u001B[38;5;241m.\u001B[39msavetxt(directory \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtgt-val.txt\u001B[39m\u001B[38;5;124m'\u001B[39m, tgt_val, fmt\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m, newline\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39mlinesep)\n\u001B[1;32m      7\u001B[0m np\u001B[38;5;241m.\u001B[39msavetxt(directory \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtgt-test.txt\u001B[39m\u001B[38;5;124m'\u001B[39m, tgt_test, fmt\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m, newline\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39mlinesep)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# save target files to disk\n",
    "\n",
    "directory = \"../data/preprocessed/\"\n",
    "\n",
    "np.savetxt(directory + 'tgt-train.txt', tgt_train, fmt='%s', newline=os.linesep)\n",
    "np.savetxt(directory + 'tgt-val.txt', tgt_val, fmt='%s', newline=os.linesep)\n",
    "np.savetxt(directory + 'tgt-test.txt', tgt_test, fmt='%s', newline=os.linesep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T19:28:58.340702459Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# save reference files to disk\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43msrc_train\u001B[49m\u001B[38;5;241m.\u001B[39mto_csv(directory \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mref_train.tsv\u001B[39m\u001B[38;5;124m'\u001B[39m, sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m'\u001B[39m, index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      4\u001B[0m src_val\u001B[38;5;241m.\u001B[39mto_csv(directory \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mref_val.tsv\u001B[39m\u001B[38;5;124m'\u001B[39m, sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m'\u001B[39m, index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      5\u001B[0m src_test\u001B[38;5;241m.\u001B[39mto_csv(directory \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mref_test.tsv\u001B[39m\u001B[38;5;124m'\u001B[39m, sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m'\u001B[39m, index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'src_train' is not defined"
     ]
    }
   ],
   "source": [
    "# save reference files to disk\n",
    "\n",
    "src_train.to_csv(directory + 'ref_train.tsv', sep='\\t', index = False)\n",
    "src_val.to_csv(directory + 'ref_val.tsv', sep='\\t', index = False)\n",
    "src_test.to_csv(directory + 'ref_test.tsv', sep='\\t', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
